{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f3623a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kaliv\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kaliv\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\kaliv\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 222ms/step\n",
      "1/1 [==============================] - 0s 243ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        gender = predict_gender(face)\n",
    "        emotion = predict_emotion(face)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'Gender: {gender}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3052ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 1s 647ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 220ms/step\n",
      "1/1 [==============================] - 0s 232ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 216ms/step\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "1/1 [==============================] - 0s 238ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 247ms/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 239ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 267ms/step\n",
      "1/1 [==============================] - 0s 241ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 226ms/step\n",
      "1/1 [==============================] - 0s 223ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 221ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "age_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_age.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\age_net.caffemodel\")\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "# Function to predict age\n",
    "def predict_age(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    age_model.setInput(blob)\n",
    "    age = age_model.forward()[0][0]\n",
    "    return age\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to predict dress color\n",
    "def predict_dress_color(frame, x, y, w, h):\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "    blurred = cv2.GaussianBlur(roi, (5, 5), 0)\n",
    "    hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "def predict_dress_color(frame, x, y, w, h):\n",
    "    roi = frame[y:y+h, x:x+w]\n",
    "    blurred = cv2.GaussianBlur(roi, (5, 5), 0)\n",
    "    hsv = cv2.cvtColor(blurred, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define color ranges for different colors\n",
    "    color_ranges = {\n",
    "        'red': ([0, 100, 100], [10, 255, 255]),\n",
    "        'orange': ([11, 100, 100], [25, 255, 255]),\n",
    "        'yellow': ([26, 100, 100], [40, 255, 255]),\n",
    "        'green': ([41, 100, 100], [80, 255, 255]),\n",
    "        'cyan': ([81, 100, 100], [100, 255, 255]),\n",
    "        'blue': ([101, 100, 100], [130, 255, 255]),\n",
    "        'purple': ([131, 100, 100], [155, 255, 255]),\n",
    "        'pink': ([156, 100, 100], [175, 255, 255]),\n",
    "        'brown': ([5, 50, 50], [20, 255, 255]),  # Adjust this range as needed\n",
    "        'white': ([0, 0, 200], [180, 50, 255]),\n",
    "        'black': ([0, 0, 0], [180, 255, 30]),\n",
    "        'gray': ([0, 0, 31], [180, 30, 220])\n",
    "    }\n",
    "\n",
    "    for color_name, (lower, upper) in color_ranges.items():\n",
    "        lower = np.array(lower, dtype=np.uint8)\n",
    "        upper = np.array(upper, dtype=np.uint8)\n",
    "        mask = cv2.inRange(hsv, lower, upper)\n",
    "        cnts = cv2.findContours(mask.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)[-2]\n",
    "        if len(cnts) > 0:\n",
    "            return color_name\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "#address=\"https://192.168.1.42:8080\"\n",
    "#cap.open(address)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        age = predict_age(frame[y:y+h, x:x+w])\n",
    "        gender = predict_gender(frame[y:y+h, x:x+w])\n",
    "        emotion = predict_emotion(frame[y:y+h, x:x+w])\n",
    "        dress_color = predict_dress_color(frame, x, y, w, h)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        #cv2.putText(frame, f'Age: {int(age)}', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Gender: {gender}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        if dress_color:\n",
    "            cv2.putText(frame, f'Dress Color: {dress_color}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4acdb950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.42s/it]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.13it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.27it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:02<00:00,  1.48it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.06s/it]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.21it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.30it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.26s/it]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.29it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.26it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.56it/s]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:06<00:00,  1.56s/it]\n",
      "Action: race: 100%|██████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from deepface import DeepFace\n",
    "import time\n",
    "\n",
    "# Define age ranges\n",
    "age_ranges = [\n",
    "    (0, 5),\n",
    "    (6, 10),\n",
    "    (11, 15),\n",
    "    (16, 20),\n",
    "    (21, 25),\n",
    "    (26, 30),\n",
    "    (31, 35),\n",
    "    (36, 40),\n",
    "    (41, 45),\n",
    "    (46, 50),\n",
    "    (51, 55),\n",
    "    (56, 60),\n",
    "    (61, 65),\n",
    "    (66, 70),\n",
    "    (71, 75),\n",
    "    (76, 80),\n",
    "    (81, 85),\n",
    "    (86, 90),\n",
    "    (91, 95),\n",
    "    (96, 100)\n",
    "]\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Use face_recognition library to detect faces\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    \n",
    "    # For each face, predict the age\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        face_img = frame[top:bottom, left:right]\n",
    "        \n",
    "        # Analyze the face for age prediction\n",
    "        try:\n",
    "            age_prediction = DeepFace.analyze(face_img, enforce_detection=False)[0]['age']\n",
    "            \n",
    "            # Categorize age into predefined ranges\n",
    "            age_category = \"N/A\"\n",
    "            for age_range in age_ranges:\n",
    "                if age_range[0] <= age_prediction <= age_range[1]:\n",
    "                    age_category = f\"{age_range[0]}-{age_range[1]}\"\n",
    "                    break\n",
    "        except (ValueError, IndexError):\n",
    "            age_category = \"N/A\"\n",
    "        \n",
    "        # Draw age prediction on the frame\n",
    "        cv2.putText(frame, f'Age: {age_category}', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Draw rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "    \n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Webcam Age Estimation', frame)\n",
    "    \n",
    "    # Introduce a delay for smoother camera feed\n",
    "    time.sleep(0.05)\n",
    "    \n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b08629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "age_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_age.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\age_net.caffemodel\")\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict age\n",
    "def predict_age(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    age_model.setInput(blob)\n",
    "    age = age_model.forward()[0][0]\n",
    "    return age\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Get user name and registration number\n",
    "name = input(\"Enter your name: \")\n",
    "reg_number = input(\"Enter your registration number: \")\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        age = predict_age(face)\n",
    "        gender = predict_gender(face)\n",
    "        emotion = predict_emotion(face)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'Name: {name}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Registration Number: {reg_number}', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Gender: {gender}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dda3621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            name = \"Unknown\"\n",
    "            \n",
    "            # Check if any known face matches\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "                \n",
    "            # Display name instead of gender and emotion\n",
    "            cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # Face not recognized, predict gender and emotion\n",
    "            gender = predict_gender(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1dcaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            name = \"Unknown\"\n",
    "            \n",
    "            # Check if any known face matches\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "                \n",
    "            # Display name instead of gender and emotion\n",
    "            cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # Face not recognized, predict gender and emotion\n",
    "            gender = predict_gender(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e4f390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(rgb_frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            name = \"Unknown\"\n",
    "            \n",
    "            # Check if any known face matches\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "                \n",
    "            # Display name instead of gender, age, and emotion\n",
    "            cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # Face not recognized, predict gender, age, and emotion\n",
    "            gender = predict_gender(face)\n",
    "            age = predict_age(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            cv2.putText(frame, f'Name: Unknown', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Age: {age}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3beff59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 0s 406ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 234ms/step\n",
      "1/1 [==============================] - 0s 254ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Always predict gender and emotion\n",
    "        gender = predict_gender(face)\n",
    "        emotion = predict_emotion(face)\n",
    "\n",
    "        cv2.putText(frame, f'Name: Unknown', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df19ab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 0s 430ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            name = \"Unknown\"\n",
    "            \n",
    "            # Check if any known face matches\n",
    "            if True in matches:\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "                \n",
    "            # Display name instead of gender and emotion\n",
    "            cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            gender = predict_gender(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            \n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    else:\n",
    "            # Face not recognized, predict gender and emotion\n",
    "            gender = predict_gender(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            cv2.putText(frame, f'Name: Unknown', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "    cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "153cc66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if True in matches:\n",
    "                # Known face recognized, get the name\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "            else:\n",
    "                # Unknown face, set name as \"Unknown\"\n",
    "                name = \"Unknown\"\n",
    "                \n",
    "            # Display name along with gender and emotion\n",
    "            cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        else:\n",
    "            # Face not recognized, predict gender and emotion\n",
    "            gender = predict_gender(face)\n",
    "            emotion = predict_emotion(face)\n",
    "\n",
    "            #cv2.putText(frame, f'Name: Unknown', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            #cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            #cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "356fa1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 0s 397ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 194ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 281ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import os\n",
    "\n",
    "# Load pre-trained models\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "gender_model = cv2.dnn.readNet(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\deploy_gender.prototxt\", r\"C:\\Users\\kaliv\\Downloads\\ikkada\\gender_net.caffemodel\")\n",
    "emotion_model = load_model(r\"C:\\Users\\kaliv\\Downloads\\ikkada\\emotion_model.hdf5\")\n",
    "\n",
    "# Function to predict gender\n",
    "def predict_gender(face):\n",
    "    blob = cv2.dnn.blobFromImage(face, 1, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)\n",
    "    gender_model.setInput(blob)\n",
    "    gender = \"Male\" if gender_model.forward()[0][0] > 0.5 else \"Female\"\n",
    "    return gender\n",
    "\n",
    "# Function to predict emotion\n",
    "def predict_emotion(face):\n",
    "    face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "    face = cv2.resize(face, (64, 64))  # Resize to (64, 64)\n",
    "    face = face.astype(\"float\") / 255.0\n",
    "    face = np.expand_dims(face, axis=0)\n",
    "    face = np.expand_dims(face, axis=-1)\n",
    "\n",
    "    emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    prediction = emotion_model.predict(face)\n",
    "    return emotion_labels[np.argmax(prediction)]\n",
    "\n",
    "# Function to collect known faces and their names\n",
    "def collect_known_faces():\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "    # Example: Collect known faces from images\n",
    "    known_faces_dir = r\"C:\\Users\\kaliv\\Downloads\\ikkada\\known_faces\"\n",
    "    for filename in os.listdir(known_faces_dir):\n",
    "        image_path = os.path.join(known_faces_dir, filename)\n",
    "        if os.path.isfile(image_path):\n",
    "            image = face_recognition.load_image_file(image_path)\n",
    "            encoding = face_recognition.face_encodings(image)[0]\n",
    "            known_face_encodings.append(encoding)\n",
    "            # Extract name from filename (assuming filename is in format \"name.jpg\")\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            known_face_names.append(name)\n",
    "\n",
    "    return known_face_encodings, known_face_names\n",
    "\n",
    "# Load known faces and their names\n",
    "known_face_encodings, known_face_names = collect_known_faces()\n",
    "\n",
    "# Capture video from webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Encode the face for recognition\n",
    "        face_encodings = face_recognition.face_encodings(frame, [(y, x+w, y+h, x)])\n",
    "        \n",
    "        if len(face_encodings) > 0:\n",
    "            # Face recognized\n",
    "            matches = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "            \n",
    "            if True in matches:\n",
    "                # Known face recognized, get the name\n",
    "                first_match_index = matches.index(True)\n",
    "                name = known_face_names[first_match_index]\n",
    "                \n",
    "                # Display name\n",
    "                cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            else:\n",
    "                # Unknown face, set name as \"Unknown\"\n",
    "                name = \"Unknown\"\n",
    "                \n",
    "                # Predict gender and emotion\n",
    "                gender = predict_gender(face)\n",
    "                emotion = predict_emotion(face)\n",
    "\n",
    "                # Display name, gender, and emotion\n",
    "                cv2.putText(frame, f'Name: {name}', (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'Gender: {gender}', (x, y+h+50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f'Emotion: {emotion}', (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc49e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
